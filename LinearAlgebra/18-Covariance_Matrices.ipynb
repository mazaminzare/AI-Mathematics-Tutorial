{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Covariance Matrices: An In-Depth Tutorial\n",
        "\n",
        "#### Mathematical Background\n",
        "\n",
        "A covariance matrix is a key concept in statistics and linear algebra, used to measure the extent to which two random variables change together. It provides a succinct way to capture the variance and covariance of multiple random variables.\n",
        "\n",
        "#### Key Concepts in Covariance Matrices\n",
        "\n",
        "1. **Covariance**:\n",
        "    - The covariance between two random variables \\(X\\) and \\(Y\\) is defined as:\n",
        "    $$\n",
        "    \\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\n",
        "    $$\n",
        "    where \\(\\mathbb{E}\\) denotes the expected value.\n",
        "\n",
        "2. **Variance**:\n",
        "    - The variance of a random variable \\(X\\) is a special case of covariance where \\(Y = X\\):\n",
        "    $$\n",
        "    \\text{Var}(X) = \\text{Cov}(X, X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n",
        "    $$\n",
        "\n",
        "3. **Covariance Matrix**:\n",
        "    - For a random vector \\(\\mathbf{X} = [X_1, X_2, \\ldots, X_n]^\\top\\), the covariance matrix \\(\\Sigma\\) is defined as:\n",
        "    $$\n",
        "    \\Sigma = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])^\\top]\n",
        "    $$\n",
        "    - The element \\(\\sigma_{ij}\\) of \\(\\Sigma\\) represents the covariance between \\(X_i\\) and \\(X_j\\).\n",
        "\n",
        "#### Properties of Covariance Matrices\n",
        "\n",
        "1. **Symmetry**:\n",
        "    - Covariance matrices are symmetric, meaning \\(\\sigma_{ij} = \\sigma_{ji}\\).\n",
        "\n",
        "2. **Positive Semi-Definiteness**:\n",
        "    - Covariance matrices are positive semi-definite, implying that for any vector \\(\\mathbf{a}\\),\n",
        "    $$\n",
        "    \\mathbf{a}^\\top \\Sigma \\mathbf{a} \\geq 0\n",
        "    $$\n",
        "\n",
        "3. **Diagonal Elements**:\n",
        "    - The diagonal elements of a covariance matrix represent the variances of the individual random variables:\n",
        "    $$\n",
        "    \\sigma_{ii} = \\text{Var}(X_i)\n",
        "    $$\n",
        "\n",
        "4. **Zero Covariance**:\n",
        "    - If the covariance between two random variables is zero, they are uncorrelated:\n",
        "    $$\n",
        "    \\text{Cov}(X_i, X_j) = 0 \\implies X_i \\text{ and } X_j \\text{ are uncorrelated}\n",
        "    $$\n",
        "\n",
        "#### Numerical Example\n",
        "\n",
        "Consider a dataset with two random variables \\(X\\) and \\(Y\\), with the following data points:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ccc}\n",
        "X & Y \\\\\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5 \\\\\n",
        "5 & 6 \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "1. **Calculate the Means**:\n",
        "    - Mean of \\(X\\):\n",
        "    $$\n",
        "    \\bar{X} = \\frac{1+2+3+4+5}{5} = 3\n",
        "    $$\n",
        "    - Mean of \\(Y\\):\n",
        "    $$\n",
        "    \\bar{Y} = \\frac{2+3+4+5+6}{5} = 4\n",
        "    $$\n",
        "\n",
        "2. **Calculate the Covariance**:\n",
        "    - Covariance between \\(X\\) and \\(Y\\):\n",
        "    $$\n",
        "    \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\n",
        "    $$\n",
        "    - Plugging in the values:\n",
        "    $$\n",
        "    \\text{Cov}(X, Y) = \\frac{1}{4} \\left[(1-3)(2-4) + (2-3)(3-4) + (3-3)(4-4) + (4-3)(5-4) + (5-3)(6-4)\\right] = \\frac{1}{4} \\left[4 + 1 + 0 + 1 + 4\\right] = \\frac{10}{4} = 2.5\n",
        "    $$\n",
        "\n",
        "3. **Form the Covariance Matrix**:\n",
        "    - Variance of \\(X\\):\n",
        "    $$\n",
        "    \\text{Var}(X) = \\text{Cov}(X, X) = \\frac{1}{4} \\left[(1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2\\right] = \\frac{1}{4} \\left[4 + 1 + 0 + 1 + 4\\right] = 2.5\n",
        "    $$\n",
        "    - Variance of \\(Y\\):\n",
        "    $$\n",
        "    \\text{Var}(Y) = \\text{Cov}(Y, Y) = \\frac{1}{4} \\left[(2-4)^2 + (3-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2\\right] = \\frac{1}{4} \\left[4 + 1 + 0 + 1 + 4\\right] = 2.5\n",
        "    $$\n",
        "    - Covariance matrix:\n",
        "    $$\n",
        "    \\Sigma = \\begin{bmatrix}\n",
        "    \\text{Var}(X) & \\text{Cov}(X, Y) \\\\\n",
        "    \\text{Cov}(X, Y) & \\text{Var}(Y)\n",
        "    \\end{bmatrix} = \\begin{bmatrix}\n",
        "    2.5 & 2.5 \\\\\n",
        "    2.5 & 2.5\n",
        "    \\end{bmatrix}\n",
        "\n",
        "\n",
        "#### Key Properties of Covariance Matrices\n",
        "\n",
        "1. **Symmetry**: $\\Sigma$ is symmetric, i.e., $\\sigma_{ij} = \\sigma_{ji}$.\n",
        "2. **Positive Semi-Definiteness**: For any vector $\\mathbf{a}$, $\\mathbf{a}^\\top \\Sigma \\mathbf{a} \\geq 0$.\n",
        "3. **Diagonal Elements**: Diagonal elements $\\sigma_{ii}$ represent the variances of the variables $X_i$.\n",
        "4. **Uncorrelated Variables**: If $\\text{Cov}(X_i, X_j) = 0$, $X_i$ and $X_j$ are uncorrelated.\n",
        "\n",
        "#### Important Notes on Using Covariance Matrices\n",
        "\n",
        "- **Data Centering**: When calculating the covariance matrix from data, it is essential to center the data by subtracting the mean of each variable.\n",
        "- **Multivariate Normal Distribution**: The covariance matrix plays a crucial role in the multivariate normal distribution, where it defines the shape of the distribution.\n",
        "- **Principal Component Analysis (PCA)**: Covariance matrices are fundamental in PCA, which uses eigenvectors and eigenvalues of the covariance matrix to reduce dimensionality and identify principal components.\n",
        "- **Noise Sensitivity**: Covariance matrices can be sensitive to noise and outliers in the data, potentially affecting the accuracy of subsequent analyses.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sPr8DDMbr3wo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rm8O14urzBa",
        "outputId": "4ee62fbe-1b37-4095-dd33-25c4a3aaa324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            "[[2.5 2.5]\n",
            " [2.5 2.5]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data: each row is an observation, each column is a variable\n",
        "data = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 4],\n",
        "    [4, 5],\n",
        "    [5, 6]\n",
        "])\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "cov_matrix = np.cov(data, rowvar=False)\n",
        "\n",
        "print(\"Covariance Matrix:\")\n",
        "print(cov_matrix)"
      ]
    }
  ]
}
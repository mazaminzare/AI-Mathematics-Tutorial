{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Projections: An In-Depth Tutorial\n",
        "\n",
        "#### Mathematical Background\n",
        "\n",
        "Projections are an essential concept in linear algebra, used extensively in vector spaces, geometry, and data analysis. Projections allow us to map vectors onto subspaces, simplifying problems and reducing dimensions while retaining significant features.\n",
        "\n",
        "##### Projection onto a Line\n",
        "\n",
        "Consider a vector $\\mathbf{v}$ and a unit vector $\\mathbf{u}$ in $\\mathbb{R}^n$. The projection of $\\mathbf{v}$ onto $\\mathbf{u}$ is given by:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{\\mathbf{u}} \\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{u}) \\mathbf{u}\n",
        "$$\n",
        "\n",
        "Here, $\\mathbf{v} \\cdot \\mathbf{u}$ is the dot product of $\\mathbf{v}$ and $\\mathbf{u}$, and $\\mathbf{u}$ is a unit vector, ensuring the projection scales correctly along the direction of $\\mathbf{u}$.\n",
        "\n",
        "##### Projection onto a Subspace\n",
        "\n",
        "Consider a subspace $W$ of $\\mathbb{R}^n$ spanned by an orthonormal set of vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_k\\}$. The projection of a vector $\\mathbf{v}$ onto $W$ is the sum of the projections onto each vector in the basis:\n",
        "\n",
        "$$\n",
        "\\text{proj}_W \\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{u}_1) \\mathbf{u}_1 + (\\mathbf{v} \\cdot \\mathbf{u}_2) \\mathbf{u}_2 + \\cdots + (\\mathbf{v} \\cdot \\mathbf{u}_k) \\mathbf{u}_k\n",
        "$$\n",
        "\n",
        "This formula assumes $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_k\\}$ is an orthonormal basis for $W$.\n",
        "\n",
        "##### Properties of Projections\n",
        "\n",
        "1. **Idempotent Property**: Applying the projection operator twice yields the same result as applying it once. For a projection operator $P$, this property is expressed as $P^2 = P$.\n",
        "\n",
        "2. **Orthogonality**: The vector $\\mathbf{v}$ can be decomposed into two orthogonal components: one in the subspace $W$ and one orthogonal to $W$. If $\\mathbf{v} = \\mathbf{v}_W + \\mathbf{v}_{W^\\perp}$, then $\\mathbf{v}_W = \\text{proj}_W \\mathbf{v}$ and $\\mathbf{v}_{W^\\perp}$ is orthogonal to $W$.\n",
        "\n",
        "3. **Linear Transformation**: Projection is a linear transformation. If $P$ is a projection operator, then for any vectors $\\mathbf{v}$ and $\\mathbf{w}$, and scalars $a$ and $b$, we have $P(a\\mathbf{v} + b\\mathbf{w}) = aP(\\mathbf{v}) + bP(\\mathbf{w})$.\n",
        "\n",
        "##### Numerical Example\n",
        "\n",
        "Consider the vector $\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$ and the unit vector $\\mathbf{u} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}$.\n",
        "\n",
        "The projection of $\\mathbf{v}$ onto $\\mathbf{u}$ is:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{\\mathbf{u}} \\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{u}) \\mathbf{u}\n",
        "$$\n",
        "\n",
        "First, compute the dot product $\\mathbf{v} \\cdot \\mathbf{u}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{v} \\cdot \\mathbf{u} = 3 \\cdot \\frac{1}{\\sqrt{2}} + 4 \\cdot \\frac{1}{\\sqrt{2}} = \\frac{3}{\\sqrt{2}} + \\frac{4}{\\sqrt{2}} = \\frac{7}{\\sqrt{2}}\n",
        "$$\n",
        "\n",
        "Now, compute the projection:\n",
        "\n",
        "$$\n",
        "\\text{proj}_{\\mathbf{u}} \\mathbf{v} = \\frac{7}{\\sqrt{2}} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} = \\frac{7}{2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{7}{2} \\\\ \\frac{7}{2} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The projection of $\\mathbf{v}$ onto $\\mathbf{u}$ is $\\begin{bmatrix} \\frac{7}{2} \\\\ \\frac{7}{2} \\end{bmatrix}$.\n",
        "\n",
        "##### Projection onto a Plane\n",
        "\n",
        "Now, consider projecting a vector onto a plane spanned by two orthonormal vectors $\\mathbf{u}_1$ and $\\mathbf{u}_2$. Let:\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{u}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and $\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}$.\n",
        "\n",
        "The projection of $\\mathbf{v}$ onto the plane $W$ spanned by $\\mathbf{u}_1$ and $\\mathbf{u}_2$ is:\n",
        "\n",
        "$$\n",
        "\\text{proj}_W \\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{u}_1) \\mathbf{u}_1 + (\\mathbf{v} \\cdot \\mathbf{u}_2) \\mathbf{u}_2\n",
        "$$\n",
        "\n",
        "First, compute the dot products:\n",
        "\n",
        "$$\n",
        "\\mathbf{v} \\cdot \\mathbf{u}_1 = 3, \\quad \\mathbf{v} \\cdot \\mathbf{u}_2 = 4\n",
        "$$\n",
        "\n",
        "Now, compute the projection:\n",
        "\n",
        "$$\n",
        "\\text{proj}_W \\mathbf{v} = 3 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + 4 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 4 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The projection of $\\mathbf{v}$ onto the plane $W$ is $\\begin{bmatrix} 3 \\\\ 4 \\\\ 0 \\end{bmatrix}$.\n",
        "\n",
        "##### Important Notes on Using Projections\n",
        "\n",
        "- **Least Squares Solutions**: Projections are crucial in finding least squares solutions to linear systems. If $A \\mathbf{x} = \\mathbf{b}$ is overdetermined, the least squares solution minimizes the distance from $\\mathbf{b}$ to the column space of $A$.\n",
        "\n",
        "- **Dimensionality Reduction**: Projections reduce the dimensionality of data while preserving its structure. This is essential in Principal Component Analysis (PCA) and other data reduction techniques.\n",
        "\n",
        "- **Orthogonal Decomposition**: Any vector can be decomposed into a component within a subspace and a component orthogonal to it. This decomposition is fundamental in many applications, including signal processing and statistics.\n",
        "\n",
        "This tutorial provides a comprehensive overview of projections, demonstrating their importance and application in linear algebra.\n"
      ],
      "metadata": {
        "id": "4z2BOJRs40LM"
      }
    }
  ]
}
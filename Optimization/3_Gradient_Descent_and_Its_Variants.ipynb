{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent and Its Variants\n",
        "\n",
        "Gradient descent is a fundamental optimization algorithm used to minimize an objective function, typically the loss function in machine learning models, by iteratively moving towards the minimum of the function.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "### Basic Gradient Descent\n",
        "\n",
        "The basic idea of gradient descent is to update the parameters of the model in the opposite direction of the gradient of the objective function with respect to the parameters. The learning rate $\\eta$ determines the size of the steps taken towards the minimum.\n",
        "\n",
        "The update rule for gradient descent is given by:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\theta_t$ are the parameters at iteration $t$.\n",
        "- $\\eta$ is the learning rate.\n",
        "- $\\nabla_{\\theta} L(\\theta_t)$ is the gradient of the loss function $L$ with respect to $\\theta_t$.\n",
        "\n",
        "**Advantages**:\n",
        "- Simple and easy to implement.\n",
        "- Suitable for convex problems.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Can be slow for large datasets.\n",
        "- Requires careful tuning of the learning rate.\n",
        "- May get stuck in local minima for non-convex problems.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "In stochastic gradient descent, the gradient is computed using a single sample (or a small batch) instead of the entire dataset. This can lead to faster convergence but with more noise in the updates.\n",
        "\n",
        "The update rule for SGD is:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t; x_i, y_i)\n",
        "$$\n",
        "\n",
        "where $(x_i, y_i)$ is a single training example.\n",
        "\n",
        "**Advantages**:\n",
        "- Faster iterations, suitable for large datasets.\n",
        "- Introduces noise which can help escape local minima.\n",
        "\n",
        "**Disadvantages**:\n",
        "- The updates are noisy, which can lead to convergence issues.\n",
        "- Requires careful tuning of the learning rate and batch size.\n",
        "\n",
        "### Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-batch gradient descent is a compromise between batch gradient descent and stochastic gradient descent. It computes the gradient using a small batch of training examples.\n",
        "\n",
        "The update rule for mini-batch gradient descent is:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_{\\theta} L(\\theta_t; x_i, y_i)\n",
        "$$\n",
        "\n",
        "where $m$ is the batch size.\n",
        "\n",
        "**Advantages**:\n",
        "- Reduces the variance of the updates, leading to more stable convergence.\n",
        "- Can leverage the benefits of vectorized operations on modern hardware.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Still requires careful tuning of the learning rate and batch size.\n",
        "- Can be slower than SGD per iteration.\n",
        "\n",
        "## Variants of Gradient Descent\n",
        "\n",
        "### Momentum\n",
        "\n",
        "Momentum helps accelerate gradients vectors in the right directions, thus leading to faster converging. It introduces a momentum term that accumulates the past gradients to smooth out the updates.\n",
        "\n",
        "The update rule with momentum is:\n",
        "\n",
        "$$\n",
        "v_{t} = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - v_t\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the momentum term.\n",
        "\n",
        "**Advantages**:\n",
        "- Speeds up convergence, especially in the presence of high curvature, small but consistent gradients, or noisy gradients.\n",
        "- Reduces oscillations.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires tuning of an additional hyperparameter (momentum term $\\gamma$).\n",
        "\n",
        "### Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "NAG is a variant of momentum that looks ahead by calculating the gradient at the approximate future position of the parameters.\n",
        "\n",
        "The update rule for NAG is:\n",
        "\n",
        "$$\n",
        "v_{t} = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} L(\\theta_t - \\gamma v_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - v_t\n",
        "$$\n",
        "\n",
        "**Advantages**:\n",
        "- Provides more accurate updates by looking ahead.\n",
        "- Can lead to faster convergence than standard momentum.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires tuning of an additional hyperparameter (momentum term $\\gamma$).\n",
        "\n",
        "### Adagrad\n",
        "\n",
        "Adagrad adapts the learning rate for each parameter individually based on the historical gradients, which can be useful for sparse data.\n",
        "\n",
        "The update rule for Adagrad is:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t,ii} + \\epsilon}} \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "where $G_t$ is a diagonal matrix where each diagonal element $i, i$ is the sum of the squares of the gradients with respect to $\\theta_i$ up to time step $t$, and $\\epsilon$ is a small constant to avoid division by zero.\n",
        "\n",
        "**Advantages**:\n",
        "- Automatically adjusts the learning rate for each parameter.\n",
        "- Effective for sparse data.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Learning rate can become too small, stopping learning prematurely.\n",
        "- Accumulation of squared gradients can lead to slow convergence.\n",
        "\n",
        "### RMSprop\n",
        "\n",
        "RMSprop modifies Adagrad to perform better in online and non-stationary settings by using a moving average of the squared gradients.\n",
        "\n",
        "The update rule for RMSprop is:\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\rho E[g^2]_{t-1} + (1 - \\rho) \\nabla_{\\theta} L(\\theta_t)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "where $\\rho$ is the decay rate.\n",
        "\n",
        "**Advantages**:\n",
        "- Adapts learning rate like Adagrad but without diminishing it too much.\n",
        "- Effective for non-stationary objectives.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires tuning of additional hyperparameters (decay rate $\\rho$).\n",
        "\n",
        "### Adam\n",
        "\n",
        "Adam combines the ideas of momentum and RMSprop. It computes adaptive learning rates for each parameter and keeps an exponentially decaying average of past gradients.\n",
        "\n",
        "The update rules for Adam are:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} L(\\theta_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_{\\theta} L(\\theta_t))^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "where $m_t$ and $v_t$ are estimates of the first moment (mean) and the second moment (uncentered variance) of the gradients, respectively. $\\beta_1$ and $\\beta_2$ are decay rates for these moment estimates.\n",
        "\n",
        "**Advantages**:\n",
        "- Combines the benefits of RMSprop and momentum.\n",
        "- Well-suited for problems with sparse gradients and non-stationary objectives.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Requires tuning of additional hyperparameters ($\\beta_1$, $\\beta_2$).\n",
        "- Can be computationally expensive due to maintaining multiple moving averages.\n",
        "\n",
        "## Practical Considerations\n",
        "\n",
        "### Learning Rate Scheduling\n",
        "\n",
        "Adjusting the learning rate during training can lead to better performance. Common schedules include:\n",
        "- **Step Decay**: Reducing the learning rate by a factor every few epochs.\n",
        "- **Exponential Decay**: Reducing the learning rate exponentially over time.\n",
        "- **Warm Restarts**: Periodically resetting the learning rate to a higher value.\n",
        "\n",
        "### Choosing the Right Optimizer\n",
        "\n",
        "Different optimizers have their own strengths and weaknesses. The choice of optimizer can depend on the specific problem, the architecture of the model, and the dataset. Experimentation and empirical testing are often required to determine the best optimizer for a given task.\n",
        "\n"
      ],
      "metadata": {
        "id": "dImmx-8SL3rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CklCJV8KLEuE"
      },
      "outputs": [],
      "source": []
    }
  ]
}
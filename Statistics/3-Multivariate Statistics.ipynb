{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Statistics\n",
        "\n",
        "Multivariate statistics involve the observation and analysis of more than one statistical outcome variable at a time. This tutorial covers key concepts, mathematical background, and numerical examples.\n",
        "\n",
        "### 1. Multiple Linear Regression\n",
        "\n",
        "Multiple linear regression models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we want to predict students' test scores ($y$) based on both study hours ($x_1$) and attendance ($x_2$). We have the following data:\n",
        "\n",
        "| Study Hours ($x_1$) | Attendance ($x_2$) | Test Scores ($y$) |\n",
        "|---------------------|---------------------|-------------------|\n",
        "| 2                   | 90                  | 65                |\n",
        "| 3                   | 95                  | 70                |\n",
        "| 4                   | 100                 | 75                |\n",
        "| 5                   | 105                 | 80                |\n",
        "\n",
        "The multiple linear regression equation is:\n",
        "\n",
        "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon $$\n",
        "\n",
        "Where:\n",
        "- $y$ = Test Scores (dependent variable)\n",
        "- $x_1$ = Study Hours (independent variable)\n",
        "- $x_2$ = Attendance (independent variable)\n",
        "- $\\beta_0$ = Intercept\n",
        "- $\\beta_1$, $\\beta_2$ = Slopes for $x_1$ and $x_2$\n",
        "- $\\epsilon$ = Error term\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Interpretation of Coefficients**: Each $\\beta_i$ represents the change in $y$ for a one-unit change in $x_i$, holding other variables constant.\n",
        "2. **Multicollinearity**: When independent variables are highly correlated, it can affect the stability and interpretation of the coefficients.\n",
        "3. **Model Fit**: Evaluated using $R^2$, adjusted $R^2$, and other goodness-of-fit measures.\n",
        "\n",
        "### 2. Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of variables in a dataset while retaining most of the variability.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have a dataset with four variables: $X_1$, $X_2$, $X_3$, and $X_4$. The goal is to reduce these four variables into two principal components.\n",
        "\n",
        "1. **Standardize the Data**: Center the data around the mean and scale it by the standard deviation.\n",
        "\n",
        "2. **Compute the Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
        "\n",
        "3. **Compute the Eigenvalues and Eigenvectors**: The eigenvectors of the covariance matrix are the principal components, and the eigenvalues represent the variance explained by each component.\n",
        "\n",
        "4. **Form the Principal Components**: Multiply the standardized data by the eigenvectors.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Variance Explained**: The proportion of the dataset's total variance explained by each principal component.\n",
        "2. **Orthogonality**: Principal components are orthogonal (uncorrelated) to each other.\n",
        "3. **Dimensionality Reduction**: PCA reduces the dimensionality of the data while preserving as much variance as possible.\n",
        "\n",
        "### 3. Factor Analysis\n",
        "\n",
        "Factor Analysis is used to identify underlying relationships between variables by modeling them as linear combinations of potential factors.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have a dataset with four observed variables: $X_1$, $X_2$, $X_3$, and $X_4$. The goal is to identify two underlying factors.\n",
        "\n",
        "1. **Extract Initial Factors**: Use methods like Principal Axis Factoring or Maximum Likelihood to estimate the initial factors.\n",
        "\n",
        "2. **Rotate the Factors**: Apply rotations (e.g., Varimax) to make the factor loadings more interpretable.\n",
        "\n",
        "3. **Interpret the Factors**: Examine the rotated factor loadings to identify the underlying factors.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Factor Loadings**: The coefficients that represent the relationship between the observed variables and the factors.\n",
        "2. **Communality**: The proportion of each variable's variance explained by the factors.\n",
        "3. **Factor Scores**: Estimated values of the factors for each observation.\n",
        "\n",
        "### 4. Canonical Correlation Analysis (CCA)\n",
        "\n",
        "Canonical Correlation Analysis (CCA) examines the relationships between two sets of variables.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have two sets of variables:\n",
        "- Set 1: $X_1$, $X_2$, $X_3$\n",
        "- Set 2: $Y_1$, $Y_2$\n",
        "\n",
        "The goal is to find linear combinations of the $X$ variables and the $Y$ variables that are maximally correlated.\n",
        "\n",
        "1. **Compute Canonical Correlations**: Find the linear combinations of the $X$ and $Y$ variables that maximize the correlation between the sets.\n",
        "\n",
        "2. **Interpret the Canonical Variates**: Examine the canonical variates to understand the relationships between the variable sets.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Canonical Correlation**: The correlation between the canonical variates.\n",
        "2. **Redundancy Index**: The proportion of variance in one set of variables explained by the canonical variates of the other set.\n",
        "3. **Significance Testing**: Tests whether the canonical correlations are significantly different from zero.\n",
        "\n",
        "### 5. Cluster Analysis\n",
        "\n",
        "Cluster Analysis groups observations into clusters such that observations within each cluster are more similar to each other than to those in other clusters.\n",
        "\n",
        "#### K-means Clustering\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have a dataset with observations on two variables. We want to group the observations into three clusters.\n",
        "\n",
        "1. **Initialize Centroids**: Randomly select three initial centroids.\n",
        "\n",
        "2. **Assign Clusters**: Assign each observation to the nearest centroid.\n",
        "\n",
        "3. **Update Centroids**: Calculate the mean of the observations in each cluster and update the centroids.\n",
        "\n",
        "4. **Iterate**: Repeat steps 2 and 3 until the centroids no longer change significantly.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Within-Cluster Sum of Squares (WCSS)**: Measure of the variance within each cluster.\n",
        "2. **Number of Clusters**: Determined using methods like the Elbow Method.\n",
        "3. **Cluster Centroids**: The mean of the observations in each cluster.\n",
        "\n",
        "#### Hierarchical Clustering\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have a dataset with observations on two variables. We want to group the observations into a hierarchical structure.\n",
        "\n",
        "1. **Compute Distance Matrix**: Calculate the pairwise distances between observations.\n",
        "\n",
        "2. **Linkage Method**: Use a linkage method (e.g., single, complete, average) to determine the distance between clusters.\n",
        "\n",
        "3. **Merge Clusters**: Iteratively merge the closest clusters until all observations are in a single cluster.\n",
        "\n",
        "4. **Dendrogram**: Visualize the hierarchical structure using a dendrogram.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Linkage Criteria**: Method used to calculate the distance between clusters (e.g., single, complete, average).\n",
        "2. **Dendrogram**: A tree-like diagram showing the hierarchical structure of clusters.\n",
        "3. **Cut-off Point**: Determine the number of clusters by cutting the dendrogram at a specific level.\n",
        "\n",
        "### 6. Discriminant Analysis\n",
        "\n",
        "Discriminant Analysis is used to classify observations into predefined groups based on predictor variables.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose we have data on students' test scores in math and science and want to classify them into pass or fail groups.\n",
        "\n",
        "1. **Estimate Discriminant Functions**: Find linear combinations of the predictor variables that maximize the separation between the groups.\n",
        "\n",
        "2. **Classify Observations**: Assign observations to the group with the highest discriminant score.\n",
        "\n",
        "3. **Evaluate Accuracy**: Assess the classification accuracy using methods like cross-validation.\n",
        "\n",
        "**Key Properties:**\n",
        "\n",
        "1. **Discriminant Functions**: Linear combinations of predictor variables that best separate the groups.\n",
        "2. **Wilks' Lambda**: A measure of how well the discriminant function separates the groups.\n",
        "3. **Classification Accuracy**: The proportion of correctly classified observations.\n",
        "\n",
        "### 7. Summary\n",
        "\n",
        "Multivariate statistics enable the analysis of multiple variables simultaneously, providing insights into complex relationships. Multiple linear regression, PCA, factor analysis, CCA, cluster analysis, and discriminant analysis are powerful techniques for exploring and understanding multivariate data. Mastery of these concepts allows for comprehensive and effective data analysis.\n"
      ],
      "metadata": {
        "id": "_fUn3x0vXxm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLG4DwkiXw_v"
      },
      "outputs": [],
      "source": []
    }
  ]
}